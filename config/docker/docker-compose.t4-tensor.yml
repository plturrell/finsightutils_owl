version: '3.8'

# T4 Tensor Core Optimized Docker Compose configuration for OWL
# Maximizes performance for tensor core operations on NVIDIA T4 GPUs

services:
  # FastAPI application with T4 Tensor Core optimization
  api:
    build:
      context: ../..
      dockerfile: deployment/Dockerfile.t4-tensor-optimized
    image: finsight/owl-api:t4-tensor-optimized
    container_name: owl-api
    ports:
      - "8000:8000"
    environment:
      - LAYOUT_MODEL_URL=triton:8000/v2/models/nv-layoutlm-financial
      - TABLE_MODEL_URL=triton:8000/v2/models/nv-table-extraction
      - NER_MODEL_URL=triton:8000/v2/models/nv-financial-ner
      - USE_GPU=true
      - ENABLE_T4_OPTIMIZATION=true
      - ENABLE_TENSOR_CORES=true
      - USE_TF32=true
      - NVIDIA_TF32_OVERRIDE=1
      - CUDNN_TENSOR_OP_MATH=1
      - TORCH_CUDNN_V8_API_ENABLED=1
      - BASE_URI=http://finsight.dev/kg/
      - INCLUDE_PROVENANCE=true
    volumes:
      - ../../data:/app/data
    depends_on:
      - triton
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - owl-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ['0']
              count: 1
              
  # SAP OWL Converter Service with T4 Tensor Core optimization
  owl-converter:
    build:
      context: ../..
      dockerfile: deployment/Dockerfile.t4-tensor-optimized
    image: finsight/owl-converter:t4-tensor-optimized
    container_name: owl-converter
    ports:
      - "8004:8000"
    environment:
      - BASE_URI=http://finsight.dev/ontology/sap/
      - INFERENCE_LEVEL=standard
      - OUTPUT_DIR=/app/results
      - ENABLE_T4_OPTIMIZATION=true
      - ENABLE_TENSOR_CORES=true
      - USE_TF32=true
    volumes:
      - owl_results:/app/results
    restart: unless-stopped
    networks:
      - owl-network

  # Triton Inference Server with T4 Tensor Core optimization
  triton:
    image: nvcr.io/nvidia/tritonserver:23.10-py3
    container_name: owl-triton
    ports:
      - "8001:8000"
      - "8002:8001"
      - "8003:8002"
    environment:
      - LD_LIBRARY_PATH=/opt/tritonserver/lib
      - NVIDIA_TF32_OVERRIDE=1
    volumes:
      - ../../nvidia_triton/model_repository:/models
      - ../scripts/t4_tensor_core_entrypoint.sh:/opt/tritonserver/t4_tensor_core_entrypoint.sh
    command: [
      "/bin/bash", "-c",
      "/opt/tritonserver/t4_tensor_core_entrypoint.sh && tritonserver --model-repository=/models --strict-model-config=false --log-verbose=1"
    ]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 60s
    networks:
      - owl-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ['0']
              count: 1

  # Include monitoring services from base configuration
  prometheus:
    extends:
      file: docker-compose.base.yml
      service: prometheus

  grafana:
    extends:
      file: docker-compose.base.yml
      service: grafana

  dcgm-exporter:
    extends:
      file: docker-compose.base.yml
      service: dcgm-exporter

volumes:
  owl_results:
  prometheus_data:
  grafana_data:

networks:
  owl-network:
    driver: bridge